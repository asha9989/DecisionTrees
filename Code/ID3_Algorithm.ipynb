{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ID3_Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains code for ID3 Algorithm in decision trees"
      ],
      "metadata": {
        "id": "9UsFgcWT5KK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_entropy(df):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    entropy = 0\n",
        "    values = df[Class].unique()\n",
        "    for value in values:\n",
        "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
        "        entropy += -fraction*np.log2(fraction)\n",
        "    return entropy\n",
        "\n",
        "def find_entropy_attribute(df,attribute):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    target_variables = df[Class].unique()  #This gives all 'Yes' and 'No'\n",
        "    variables = df[attribute].unique()    #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
        "    entropy2 = 0\n",
        "    for variable in variables:\n",
        "        entropy = 0\n",
        "        for target_variable in target_variables:\n",
        "            num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
        "            den = len(df[attribute][df[attribute]==variable])\n",
        "            fraction = num/(den+eps)\n",
        "            entropy += -fraction*log(fraction+eps)\n",
        "        fraction2 = den/len(df)\n",
        "        entropy2 += -fraction2*entropy\n",
        "    return abs(entropy2)\n",
        "\n",
        "def find_winner(df):\n",
        "    Entropy_att = []\n",
        "    IG = []\n",
        "    for key in df.keys()[:-1]:#         Entropy_att.append(find_entropy_attribute(df,key))\n",
        "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
        "    return df.keys()[:-1][np.argmax(IG)] \n",
        "\n",
        "def get_subtable(df, node,value):\n",
        "    return df[df[node] == value].reset_index(drop=True)\n",
        "\n",
        "def buildTree(df,targetClass,tree=None): \n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name  #Here we build our decision tree  #Get attribute with maximum information gain\n",
        "    node = find_winner(df)#Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
        "    attValue = np.unique(df[node])#Create an empty dictionary to create tree    \n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}#We make loop to construct a tree by calling this function recursively. #In this we check if the subset is pure and stops if it is pure. \n",
        "    for value in attValue:\n",
        "        subtable = get_subtable(df,node,value)\n",
        "        clValue,counts = np.unique(subtable[targetClass],return_counts=True)                        \n",
        "        if len(counts)==1:#Checking purity of subset\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable,targetClass) #Calling the function recursively               \n",
        "    return tree\n",
        "\n",
        "def predict(test, tree, default=None):\n",
        "    attribute = next(iter(tree)) \n",
        "    # print(attribute) \n",
        "    if test[attribute] in tree[attribute].keys():\n",
        "        # print(tree[attribute].keys())\n",
        "        # print(test[attribute])\n",
        "        result = tree[attribute][test[attribute]]\n",
        "        if isinstance(result, dict):\n",
        "            return predict(test, result)\n",
        "        else:\n",
        "            return result\n",
        "    else:\n",
        "        return default\n"
      ],
      "metadata": {
        "id": "bh4nZFFv4Va-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Util: \n",
        " def calculateAccuracy(predictedLabels, targetLabels):\n",
        "      count=0\n",
        "      accuracy=0        \n",
        "      for i in range(len(predictedLabels)):\n",
        "          if targetLabels[i]== predictedLabels[i]:\n",
        "            count +=1\n",
        "      return (count/len(predictedLabels))*100 \n",
        "\n",
        "\n",
        " def getPerformanceMetrics(targetLabels,predictedLabels,trueLabels):\n",
        "      tp=0\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      accuracy=0\n",
        "      recall=0\n",
        "      specificity=0\n",
        "      precision=0\n",
        "      f1=0\n",
        "\n",
        "      for i in range(len(predictedLabels)):\n",
        "        if targetLabels[i]== predictedLabels[i]:\n",
        "          if targetLabels[i] in trueLabels:\n",
        "            tp +=1\n",
        "          else:\n",
        "            tn +=1\n",
        "        else:\n",
        "           if targetLabels[i] in trueLabels:\n",
        "             fn +=1\n",
        "           else:\n",
        "             fp +=1\n",
        "      accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
        "      if(tp+fn!=0):\n",
        "        recall = tp/(tp+fn)\n",
        "      if(fp+tn!=0):\n",
        "        specificity = tn/(fp+tn)\n",
        "      if(tp+fp!=0):\n",
        "        precision = tp/(tp+fp)\n",
        "      if(precision + recall!=0):\n",
        "        f1 = 2 *((precision * recall)/(precision + recall))\n",
        "      return  (accuracy, recall,specificity, precision, f1) "
      ],
      "metadata": {
        "id": "rPpCe73Ze4xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from warnings import filterwarnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import log2 as log\n",
        "import pprint\n",
        "\n",
        "eps = np.finfo(float).eps\n",
        "\n",
        "def decisionTree(df,train,targetClass):\n",
        "  tree= buildTree(train,targetClass)\n",
        "  return tree\n",
        "\n",
        "def makePrediction(tree,df,test,targetClass,trueLabels):\n",
        "  #  Perform the prediction using the built tree\n",
        "  col_names = df.columns\n",
        "  test_dict = {}\n",
        "\n",
        "  i=0;\n",
        "\n",
        "  predicted_values = []\n",
        "  target_values = []\n",
        "\n",
        "  for index, row in test.iterrows():\n",
        "      # print(row['c1'], row['c2'])\n",
        "      predicted_values.append(predict(row,tree)) \n",
        "      target_values.append(row[targetClass])   \n",
        "\n",
        "  # print('predicted_values are ' + str(predicted_values))\n",
        "  # print('target_values are ' + str(target_values))\n",
        "\n",
        "  # accuracy\n",
        "  testAccuracy = Util.calculateAccuracy(predicted_values, target_values) \n",
        "  print('Accuracy is: %f' % testAccuracy)\n",
        "\n",
        "  accuracy, recall,specificity, precision, f1 = Util.getPerformanceMetrics(target_values,predicted_values,trueLabels)\n",
        "\n",
        "  print('Performance Metrics of Test Data : Accuracy : %f , Recall : %f ,Specificity: %f , Precision : %f, F1 Score: %f' % (accuracy, recall,specificity, precision, f1) )\n",
        "\n"
      ],
      "metadata": {
        "id": "AskHqfDvpTBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call on heart dataset which has a wide range of numerical values\n",
        "\n",
        "df=pd.read_csv(\"heart.csv\")\n",
        "\n",
        "\n",
        "#split the data to train and test by using cross validation\n",
        "train, test = train_test_split(df, test_size=0.3 , random_state=1)\n",
        "\n",
        "\n",
        "tree = decisionTree(df,train,'a1p2')\n",
        "pprint.pprint(tree)\n",
        "trueLabels = [1]\n",
        "\n",
        "makePrediction(tree,df,test,'a1p2',trueLabels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMCgmQJxGFGO",
        "outputId": "01a4b8a5-b4b4-481c-f486-2f22eac34298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sc': {126: 1,\n",
            "        149: {'age': {49: 2, 71: 1}},\n",
            "        160: 1,\n",
            "        164: 2,\n",
            "        166: 2,\n",
            "        167: 2,\n",
            "        168: 1,\n",
            "        172: 2,\n",
            "        174: 2,\n",
            "        175: 1,\n",
            "        177: {'age': {43: 2, 46: 1, 59: 2, 65: 1}},\n",
            "        178: 1,\n",
            "        180: 1,\n",
            "        182: 1,\n",
            "        183: 1,\n",
            "        185: 2,\n",
            "        188: 2,\n",
            "        192: 1,\n",
            "        193: 1,\n",
            "        195: 1,\n",
            "        196: 1,\n",
            "        197: {'age': {44: 2, 53: 1}},\n",
            "        198: {'age': {35: 2, 41: 1}},\n",
            "        199: 1,\n",
            "        201: 1,\n",
            "        203: 1,\n",
            "        204: 1,\n",
            "        205: 1,\n",
            "        206: 2,\n",
            "        207: 1,\n",
            "        209: 1,\n",
            "        210: 1,\n",
            "        211: 1,\n",
            "        212: 1,\n",
            "        214: 1,\n",
            "        215: 1,\n",
            "        216: 2,\n",
            "        217: 2,\n",
            "        218: 2,\n",
            "        219: {'age': {39: 2, 44: 1, 50: 1}},\n",
            "        220: 1,\n",
            "        221: 1,\n",
            "        222: 1,\n",
            "        223: {'age': {40: 2, 67: 1}},\n",
            "        224: 2,\n",
            "        225: 2,\n",
            "        226: 1,\n",
            "        228: {'sex': {0: 2, 1: 1}},\n",
            "        229: 2,\n",
            "        230: 2,\n",
            "        231: {'age': {38: 2, 62: 1}},\n",
            "        233: {'age': {44: 1, 50: 2, 52: 1, 63: 1}},\n",
            "        234: {'age': {45: 1, 53: 1, 58: 2, 59: 1, 61: 2}},\n",
            "        236: 1,\n",
            "        237: 2,\n",
            "        239: {'mhr': {126: 2, 151: 1, 160: 1}},\n",
            "        240: 1,\n",
            "        242: 1,\n",
            "        243: {'age': {46: 1, 47: 2, 50: 2, 61: 1}},\n",
            "        244: {'age': {50: 1, 62: 2}},\n",
            "        245: 1,\n",
            "        246: {'age': {53: 1, 64: 2, 66: 2}},\n",
            "        248: {'age': {58: 1, 65: 2}},\n",
            "        249: 2,\n",
            "        250: 1,\n",
            "        254: {'age': {50: 1, 63: 2, 65: 2, 67: 2}},\n",
            "        255: 1,\n",
            "        256: 2,\n",
            "        257: 1,\n",
            "        258: 1,\n",
            "        259: 2,\n",
            "        260: {'age': {45: 1, 61: 2}},\n",
            "        263: {'age': {44: 1, 62: 2, 64: 1}},\n",
            "        264: 1,\n",
            "        265: 1,\n",
            "        266: 2,\n",
            "        267: {'age': {54: 1, 62: 2}},\n",
            "        268: 2,\n",
            "        269: {'age': {49: 1, 63: 2}},\n",
            "        270: 2,\n",
            "        271: 1,\n",
            "        273: 1,\n",
            "        274: 2,\n",
            "        275: {'age': {47: 2, 48: 1}},\n",
            "        276: 2,\n",
            "        277: 1,\n",
            "        281: 2,\n",
            "        282: 2,\n",
            "        283: {'age': {54: 2, 56: 2, 58: 1}},\n",
            "        288: {'age': {54: 1, 56: 2}},\n",
            "        289: 2,\n",
            "        290: 2,\n",
            "        295: 1,\n",
            "        298: {'age': {51: 2, 52: 1}},\n",
            "        299: 2,\n",
            "        300: 2,\n",
            "        302: 1,\n",
            "        303: 1,\n",
            "        304: 1,\n",
            "        305: 2,\n",
            "        306: 1,\n",
            "        307: 2,\n",
            "        308: 1,\n",
            "        309: 2,\n",
            "        311: 2,\n",
            "        315: 2,\n",
            "        318: 1,\n",
            "        319: 2,\n",
            "        322: 2,\n",
            "        325: 1,\n",
            "        327: 2,\n",
            "        330: 2,\n",
            "        341: 2,\n",
            "        353: 2,\n",
            "        360: 1,\n",
            "        394: 1,\n",
            "        407: 2,\n",
            "        409: 2,\n",
            "        417: 1}}\n",
            "Accuracy is: 28.395062\n",
            "Performance Metrics of Test Data : Accuracy : 0.283951 , Recall : 0.319149 ,Specificity: 0.235294 , Precision : 0.365854, F1 Score: 0.340909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call on credit dataset\n",
        "\n",
        "df = pd.read_csv('credit.csv')\n",
        "\n",
        "#This being a numerical column is decreasing accuracy of decision tree if it is chosen as a descriptive feature\n",
        "#Due to high information gain of this feature, it is being chosen as one of the descriptive features if not removed\n",
        "df = df.drop(\"amount\" , axis=1) \n",
        "df = df.drop(\"age\" , axis=1) \n",
        "\n",
        "#split the data to train and test by using cross validation\n",
        "train, test = train_test_split(df, test_size=0.3 , random_state=1)\n",
        "\n",
        "tree = decisionTree(df,train,'default')\n",
        "# pprint.pprint(tree)\n",
        "\n",
        "trueLabels = ['yes']\n",
        "makePrediction(tree,df,test,'default',trueLabels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QfRYhjYUvmi",
        "outputId": "e6e453f9-21eb-4cb1-cc85-fe2b61868e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 61.000000\n",
            "Performance Metrics of Test Data : Accuracy : 0.610000 , Recall : 0.372093 ,Specificity: 0.705607 , Precision : 0.336842, F1 Score: 0.353591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID3 Performance on Multiclass classification\n",
        "Zoo dataset - https://archive.ics.uci.edu/ml/datasets/zoo"
      ],
      "metadata": {
        "id": "U0FiVYfF1efE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call on UCI zoo dataset for multiclass\n",
        "\n",
        "df = pd.read_csv('zoo.csv',\n",
        "                      names=['animal_name','hair','feathers','eggs','milk',\n",
        "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
        "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])\n",
        "\n",
        "\n",
        "\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "df=df.drop('animal_name',axis=1)\n",
        "\n",
        "\n",
        "#split the data to train and test by using cross validation\n",
        "train, test = train_test_split(df, test_size=0.3 , random_state=1)\n",
        "\n",
        "tree = decisionTree(df,train,'class')\n",
        "# pprint.pprint(tree)\n",
        "\n",
        "trueLabels = np.unique(np.array(df.pop(\"class\").to_list()))\n",
        "\n",
        "makePrediction(tree,df,test,'class',trueLabels)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ll-HDWEbts23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb88737-6d4e-4725-ec6c-414b0e2ec64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 96.774194\n",
            "Performance Metrics of Test Data : Accuracy : 0.967742 , Recall : 0.967742 ,Specificity: 0.000000 , Precision : 1.000000, F1 Score: 0.983607\n"
          ]
        }
      ]
    }
  ]
}